image: $CI_REGISTRY/mouse-informatics/docker:latest

variables:
   # When using dind service we need to instruct docker, to talk with the
   # daemon started inside of the service. The daemon is available with
   # a network connection instead of the default /var/run/docker.sock socket.
   #
   # The 'docker' hostname is the alias of the service container as described at
   # https://docs.gitlab.com/ee/ci/docker/using_docker_images.html#accessing-the-services
   #
   # Note that if you're using the Kubernetes executor, the variable should be set to
   # tcp://localhost:2375/ because of how the Kubernetes executor connects services
   # to the job container
   # DOCKER_HOST: tcp://localhost:2375/
   #
   # For non-Kubernetes executors, we use tcp://docker:2375/
   DOCKER_HOST: tcp://docker:2375/
   # When using dind, it's wise to use the overlayfs driver for
   # improved performance.
   DOCKER_DRIVER: overlay2
   
   # Since the docker:dind container and the runner container don’t share their root
   # filesystem, the job’s working directory can be used as a mount point for children
   # containers. For example, if you have files you want to share with a child container,
   # you may create a subdirectory under /builds/$CI_PROJECT_PATH and use it as your
   # mount point.
   MOUNT_POINT: /builds/$CI_PROJECT_PATH/mnt
   
   # For EBI you need to override the definition of CI_REGISTRY to remove the port number
   CI_REGISTRY: dockerhub.ebi.ac.uk
   CI_REGISTRY_IMAGE: $CI_REGISTRY/$CI_PROJECT_PATH

   #NOW: $(date '+%Y-%m-%d-%H-%M-%S')
   #NOW: $(date '+%Y-%m-%d')
   
   # To solve the issue with the Docker in Docker 19.03 service.
   # Logged as: GitLab.com CI jobs failing if using docker:stable-dind image
   # see: https://gitlab.com/gitlab-com/gl-infra/production/issues/982
   DOCKER_TLS_CERTDIR: ""
   SCAN_KUBERNETES_MANIFESTS: "true"


# Use this command to look at your docker environment
# Note: This step can be overwritten by before_script sections in specific jobs.
#
#before_script:
#   - docker info

include:
  - template: Security/SAST.gitlab-ci.yml
  - template: Security/Secret-Detection.gitlab-ci.yml


stages:
   - download
   - mgi-formatting
   - build
   - test
   - deploy-dev
   - production-deploy
   - production-test


#
# Data download stage
#



MGI_data:
    stage: download
    before_script:
        - apk add --update curl && rm -rf /var/cache/apk/*
    script:
        #
        # The following is a useful command to list the environment variables
        # - export
        #
        #- echo ${KUBE_URL}
        #- echo ${KUBE_TOKEN}
        #- echo ${KUBE_NAMESPACE}
        #- echo ${KUBE_CA_PEM_FILE}
        #- echo ${KUBE_CA_PEM}
        #- echo ${KUBECONFIG}
        #- echo ${KUBE_INGRESS_BASE_DOMAIN}
        #- exit 1

        # Establish a data directory
        - mkdir -p "$MOUNT_POINT"
        - cd "$MOUNT_POINT"

        # MGI_Strain_data
        - curl -sSLN -O http://www.informatics.jax.org/downloads/reports/MGI_Strain.rpt

        # MGI_Allele_data
        - curl -sSLN -O http://www.informatics.jax.org/downloads/reports/KOMP_Allele.rpt
        - curl -sSLN -O http://www.informatics.jax.org/downloads/reports/EUCOMM_Allele.rpt
        - curl -sSLN -O http://www.informatics.jax.org/downloads/reports/NorCOMM_Allele.rpt
        - curl -sSLN -O http://www.informatics.jax.org/downloads/reports/MGI_PhenotypicAllele.rpt

        # MGI_Disease_data
        - curl -sSLN -O http://www.informatics.jax.org/downloads/reports/MGI_DO.rpt

    artifacts:
        paths:
             - "$MOUNT_POINT/"




orthology_db_dump:
    image: $CI_REGISTRY/mouse-informatics/postgres:11-alpine
    stage: download
    before_script:
        - apk add --update --no-cache curl
    script:
        
        # Establish a data directory
        - mkdir -p "$MOUNT_POINT"
        - cd "$MOUNT_POINT"
        
        - source "$CI_PROJECT_DIR"/scripts/orthologydb_dump.sh
        - cp "$CI_PROJECT_DIR"/schema/refdata_schema_additions.sql .
        
        
    artifacts:
        paths:
            - "$MOUNT_POINT/"



MGI-formatting:
    stage: mgi-formatting
    before_script:
        # Install iconv
        # see: https://gist.github.com/guillemcanal/be3db96d3caa315b4e2b8259cab7d07e
        - apk add --update wget build-base autoconf re2c libtool && mkdir -p /opt && cd /opt && wget http://ftp.gnu.org/pub/gnu/libiconv/libiconv-1.14.tar.gz && tar xzf libiconv-1.14.tar.gz && cd libiconv-1.14 && sed -i 's/_GL_WARN_ON_USE (gets, "gets is a security hole - use fgets instead");/#if HAVE_RAW_DECL_GETS\n_GL_WARN_ON_USE (gets, "gets is a security hole - use fgets instead");\n#endif/g' srclib/stdio.in.h && ./configure --prefix=/usr/local && make && make install && apk del wget build-base autoconf re2c libtool && rm /opt && rm -rf /var/cache/apk/* && rm -rf /usr/share/*

    script:
        - iconv -f ISO-8859-1 -t UTF-8//TRANSLIT "$MOUNT_POINT"/MGI_PhenotypicAllele.rpt > "$MOUNT_POINT"/MGI_PhenotypicAllele.rpt.test.txt
        - iconv -f ISO-8859-1 -t UTF-8//TRANSLIT "$MOUNT_POINT"/MGI_Strain.rpt > "$MOUNT_POINT"/MGI_Strain_test.rpt

    dependencies:
        - MGI_data
        - orthology_db_dump
    artifacts:
        paths:
             - "$MOUNT_POINT/"



build_image:
    stage: build
    services:
        - name: $CI_REGISTRY/mouse-informatics/dind:latest
          alias: docker
    before_script:
        - echo "${CI_REGISTRY_PASSWORD}" | docker login -u "${CI_REGISTRY_USER}" --password-stdin  ${CI_REGISTRY}
        - NOW=$(date '+%Y-%m-%d-%H-%M-%S')
        - echo "export NOW=${NOW}" > ${MOUNT_POINT}/datetime.env
    script:

        - source ${MOUNT_POINT}/datetime.env
        - echo ${NOW}


        # Build using a debian based container for Bash

        - sed -i "s|FROM postgres|FROM ${LOCAL_GITLAB_POSTGRES_IMAGE}|g" Dockerfile

        - docker build -t reference-db .  | tee ${MOUNT_POINT}/build.log
        - docker run --name refdbcontainer -v "$MOUNT_POINT:/mnt" -d reference-db

        # Time is required to load the data
        #
        # This is potential problem as the time taken to complete the data load cannot be established
        # Hence the long pause period. 
        #
        # An alternative would be to copy the data required into to the image
        # when it is built, but this is not trivial in this build system, and a downside  would be
        # that every time the container starts there would always be a lag while the data was loaded.
        #
        - sleep "${DB_LOAD_TIME}"

        - docker exec refdbcontainer sh /usr/local/data/postgres_processing_time.sh
        - docker cp refdbcontainer:/usr/local/data/database.sql "$MOUNT_POINT"/database.sql
        - docker exec refdbcontainer sh -c "rm -r /usr/local/data"
        - docker stop -t 120 refdbcontainer
        

        # Package for production using an alpine based container

        - sed -i "s|FROM postgres|FROM ${LOCAL_GITLAB_POSTGRES_IMAGE}|g" Dockerfile-production
        - docker build -t prod-reference-db -f Dockerfile-production .  | tee ${MOUNT_POINT}/build-production.log
        - docker run --name prodrefdbcontainer -v "$MOUNT_POINT:/mnt" -d prod-reference-db
        - sleep 30

        - echo "${CI_REGISTRY_IMAGE}":"${NOW}"
        
        - docker stop -t 120 prodrefdbcontainer
        - docker commit prodrefdbcontainer "${CI_REGISTRY_IMAGE}":"${NOW}"

        - rm "$MOUNT_POINT"/database.sql


        - docker tag "${CI_REGISTRY_IMAGE}":"${NOW}" "${CI_REGISTRY_IMAGE}":latest
        - docker push "${CI_REGISTRY_IMAGE}":"${NOW}"  | tee ${MOUNT_POINT}/push.log
        - docker push "${CI_REGISTRY_IMAGE}":latest  | tee ${MOUNT_POINT}/push.log

        - docker logout ${CI_REGISTRY}


        # PUSH THE IMAGE TO DOCKERHUB
        - echo "${DOCKER_HUB_PWD}" | docker login -u "${DOCKER_HUB_USER}" --password-stdin
 
        - docker tag "${CI_REGISTRY_IMAGE}":"${NOW}" "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}":"${NOW}"
        - docker push "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}":"${NOW}"  | tee ${MOUNT_POINT}/dockerhub-push-latest.log
        
        - docker tag "${CI_REGISTRY_IMAGE}":"${NOW}" "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}":latest
        - docker push "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}":latest  | tee ${MOUNT_POINT}/dockerhub-push-latest.log

        - docker logout       

    dependencies:
        - MGI-formatting
    artifacts:
        name: "database-${CI_JOB_ID}"
        paths:
            - ${MOUNT_POINT}
    



sast:
  stage: test
  script:
    - echo "Running SAST scan ..."

  artifacts:
    reports:
      container_scanning: gl-sast-report.json


secret_detection:
  stage: test
  script:
    - echo "Running secret detection scan ..."

  artifacts:
    reports:
      container_scanning: gl-secret-detection-report.json



trivy_container_scanning:
  stage: test

  services:
    - name: $CI_REGISTRY/mouse-informatics/dind:latest
      alias: docker
  
  rules:
    - if: '$CI_BUILD_REF_NAME == "master"'
      when: on_success
      allow_failure: true
  
  before_script:
    - export TRIVY_VERSION=$(wget -qO - "https://api.github.com/repos/aquasecurity/trivy/releases/latest" | grep '"tag_name":' | sed -E 's/.*"v([^"]+)".*/\1/')
    - echo $TRIVY_VERSION
    - wget --no-verbose https://github.com/aquasecurity/trivy/releases/download/v${TRIVY_VERSION}/trivy_${TRIVY_VERSION}_Linux-64bit.tar.gz -O - | tar -zxvf -
    - echo "${CI_REGISTRY_PASSWORD}" | docker login -u "${CI_REGISTRY_USER}" --password-stdin  ${CI_REGISTRY}
    
    - source ${MOUNT_POINT}/datetime.env
    - echo ${NOW}

  script:
    # Build report
    - ./trivy --exit-code 0 --cache-dir .trivycache/ --no-progress --format template --template "@contrib/gitlab.tpl" -o gl-container-scanning-report.json "${CI_REGISTRY_IMAGE}":"${NOW}"
    # Print report
    - ./trivy --exit-code 0 --cache-dir .trivycache/ --no-progress --severity HIGH "${CI_REGISTRY_IMAGE}":"${NOW}"
    # Fail on critical vulnerability
    - ./trivy --exit-code 1 --cache-dir .trivycache/ --severity CRITICAL --no-progress "${CI_REGISTRY_IMAGE}":"${NOW}"

    - docker logout ${CI_REGISTRY}

  cache:
    paths:
      - .trivycache/

  artifacts:
    reports:
      container_scanning: gl-container-scanning-report.json




    






hh-test-deploy:
  stage: deploy-dev
  # Use an ubuntu image with kubectl v1.15.2
  # An ubuntu base is required to provide a bash shell for the tests
  # rancher/hyperkube:v1.15.3-rancher1 installed on the hh cluster
  image: $CI_REGISTRY/mouse-informatics/kubectl:v1.15.2
  only:
    refs:
      - master
  script:
    - mkdir -p "$MOUNT_POINT"
    #
    - kubectl config set-cluster local --server="${HH_KUBERNETES_ENDPOINT}"
    - kubectl config set clusters.local.certificate-authority-data "${HH_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
    - kubectl config set-credentials ${HH_KUBERNETES_TEST_USER} --token="${HH_KUBERNETES_TEST_USER_TOKEN}"
    - kubectl config set-context "${HH_KUBERNETES_TEST_NAMESPACE}" --cluster=local --user=${HH_KUBERNETES_TEST_USER} --namespace="${HH_KUBERNETES_TEST_NAMESPACE}"
    - kubectl config use-context "${HH_KUBERNETES_TEST_NAMESPACE}"
    - kubectl version
    #
    # Make sure the test environment is empty of all resources
    # in case another test failed
    - kubectl delete pod,deploy,rs,svc,ing --all  -n "${HH_KUBERNETES_TEST_NAMESPACE}"
    # Wait
    - sleep "${TEST_SETUP_PAUSE}"
    
    
    - kubectl apply -f kube/wp/test/database/refdata-database-deployment.yaml
    - kubectl rollout status -f kube/wp/test/database/refdata-database-deployment.yaml
    
    - kubectl apply -f kube/wp/test/database/refdata-database-svc.yaml
    
    - kubectl apply -f kube/wp/test/hasura/hasura-deployment.yaml
    - kubectl rollout status -f kube/wp/test/hasura/hasura-deployment.yaml
    
    - kubectl apply -f kube/wp/test/hasura/hasura-svc.yaml
    - kubectl apply -f kube/wp/test/hasura/hasura-ingress.yaml
    
    
    - kubectl get pod,deployment,rs,svc,ing
    
    # Wait?
    # Run tests
    
    # Clean the test environment
    - kubectl delete -f kube/wp/test/hasura/hasura-ingress.yaml
    - kubectl delete -f kube/wp/test/hasura/hasura-svc.yaml
    - kubectl delete -f kube/wp/test/hasura/hasura-deployment.yaml
    - kubectl delete -f kube/wp/test/database/refdata-database-svc.yaml
    - kubectl delete -f kube/wp/test/database/refdata-database-deployment.yaml
    
    - kubectl get pod,deployment,rs,svc,ing
    
  artifacts:
    name: "hh-test-deploy"
    paths:
      - ${MOUNT_POINT}
    


hx-test-deploy:
  stage: deploy-dev
  # Use an ubuntu image with kubectl v1.15.2
  # An ubuntu base is required to provide a bash shell for the tests
  # rancher/hyperkube:v1.15.3-rancher1 installed on the hh cluster
  image: $CI_REGISTRY/mouse-informatics/kubectl:v1.15.2
  only:
    refs:
      - master
  script:
    - mkdir -p "$MOUNT_POINT"
    #
    - kubectl config set-cluster local --server="${HX_KUBERNETES_ENDPOINT}"
    - kubectl config set clusters.local.certificate-authority-data "${HX_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
    - kubectl config set-credentials ${HX_KUBERNETES_TEST_USER} --token="${HX_KUBERNETES_TEST_USER_TOKEN}"
    - kubectl config set-context "${HX_KUBERNETES_TEST_NAMESPACE}" --cluster=local --user=${HX_KUBERNETES_TEST_USER} --namespace="${HX_KUBERNETES_TEST_NAMESPACE}"
    - kubectl config use-context "${HX_KUBERNETES_TEST_NAMESPACE}"
    - kubectl version
    #
    # Make sure the test environment is empty of all resources
    # in case another test failed
    - kubectl delete pod,deploy,rs,svc,ing --all  -n "${HX_KUBERNETES_TEST_NAMESPACE}"
    # Wait
    - sleep "${TEST_SETUP_PAUSE}"
    
    
    - kubectl apply -f kube/wp/test/database/refdata-database-deployment.yaml
    - kubectl rollout status -f kube/wp/test/database/refdata-database-deployment.yaml
    
    - kubectl apply -f kube/wp/test/database/refdata-database-svc.yaml
    
    - kubectl apply -f kube/wp/test/hasura/hasura-deployment.yaml
    - kubectl rollout status -f kube/wp/test/hasura/hasura-deployment.yaml
    
    - kubectl apply -f kube/wp/test/hasura/hasura-svc.yaml
    - kubectl apply -f kube/wp/test/hasura/hasura-ingress.yaml
    
    
    - kubectl get pod,deployment,rs,svc,ing
    
    # Wait?
    - sleep "${TEST_SETUP_PAUSE}"
    
    # Run tests
    - source "$CI_PROJECT_DIR"/scripts/service_test.sh -s="${HX_KUBERNETES_INTERNAL_ENDPOINT}/${HX_KUBERNETES_TEST_NAMESPACE}/v1/graphql" | tee ${MOUNT_POINT}/dev-test.log
    
    
    # Clean the test environment
    - kubectl delete -f kube/wp/test/hasura/hasura-ingress.yaml
    - kubectl delete -f kube/wp/test/hasura/hasura-svc.yaml
    - kubectl delete -f kube/wp/test/hasura/hasura-deployment.yaml
    - kubectl delete -f kube/wp/test/database/refdata-database-svc.yaml
    - kubectl delete -f kube/wp/test/database/refdata-database-deployment.yaml
    
    - kubectl get pod,deployment,rs,svc,ing

  artifacts:
    name: "hx-test-deploy"
    paths:
      - ${MOUNT_POINT}
    






hh-production-deploy:
  stage: production-deploy
  # Use an image with helm v2.14.3, kubectl v1.15.2, alpine 3.10
  # rancher/hyperkube:v1.15.3-rancher1 installed on the hh cluster
  image: $CI_REGISTRY/mouse-informatics/helm-kubectl-alpine:latest
  only:
    refs:
      - master
  script:
    - source ${MOUNT_POINT}/datetime.env
    - echo ${NOW}
    #
    - kubectl config set-cluster local --server="${HH_KUBERNETES_ENDPOINT}"
    - kubectl config set clusters.local.certificate-authority-data "${HH_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
    - kubectl config set-credentials ${HH_KUBERNETES_USER} --token="${HH_KUBERNETES_USER_TOKEN}"
    - kubectl config set-context "${HH_KUBERNETES_NAMESPACE}" --cluster=local --user=${HH_KUBERNETES_USER} --namespace="${HH_KUBERNETES_NAMESPACE}"
    - kubectl config use-context "${HH_KUBERNETES_NAMESPACE}"
    - kubectl version
    #
    - sed -i "s/latest/${NOW}/g" kube/wp/production/database/refdata-database-deployment.yaml
    - sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/wp/production/database/refdata-database-deployment.yaml
    
    - |
      if kubectl apply -f kube/wp/production/database/refdata-database-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/wp/production/database/refdata-database-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi


    - kubectl rollout status -f kube/wp/production/database/refdata-database-deployment.yaml
    - kubectl get pod,deployment,rs



hx-production-deploy:
  stage: production-deploy
  # Use an image with helm v2.14.3, kubectl v1.15.2, alpine 3.10
  # rancher/hyperkube:v1.15.3-rancher1 installed on the hh cluster
  image: $CI_REGISTRY/mouse-informatics/helm-kubectl-alpine:latest
  only:
    refs:
      - master
  script:
    - source ${MOUNT_POINT}/datetime.env
    - echo ${NOW}
    #
    - kubectl config set-cluster local --server="${HX_KUBERNETES_ENDPOINT}"
    - kubectl config set clusters.local.certificate-authority-data "${HX_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
    - kubectl config set-credentials ${HX_KUBERNETES_USER} --token="${HX_KUBERNETES_USER_TOKEN}"
    - kubectl config set-context "${HX_KUBERNETES_NAMESPACE}" --cluster=local --user=${HX_KUBERNETES_USER} --namespace="${HX_KUBERNETES_NAMESPACE}"
    - kubectl config use-context "${HX_KUBERNETES_NAMESPACE}"
    - kubectl version
    #
    - sed -i "s/latest/${NOW}/g" kube/wp/production/database/refdata-database-deployment.yaml
    - sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/wp/production/database/refdata-database-deployment.yaml
    
    - |
      if kubectl apply -f kube/wp/production/database/refdata-database-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/wp/production/database/refdata-database-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi


    - kubectl rollout status -f kube/wp/production/database/refdata-database-deployment.yaml
    - kubectl get pod,deployment,rs

production-test:
  image: $CI_REGISTRY/mouse-informatics/ubuntu:latest
  stage: production-test
  before_script:
    - apt-get update && apt-get install -y curl && apt-get clean && rm -rf /var/lib/apt/lists/*
  script:
    # Establish a data directory
    - mkdir -p "$MOUNT_POINT"
    - cd "$MOUNT_POINT"
    
    - source "$CI_PROJECT_DIR"/scripts/service_test.sh -p  | tee ${MOUNT_POINT}/production-test.log
        
  artifacts:
    paths:
      - "$MOUNT_POINT/"

